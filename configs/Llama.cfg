Input("input_inds")
Embedding("input", ["input_inds"], {"out_features":hidden_size, "vocab_size":vocab_size})
Linear("q_proj", ["input"], {"out_features":hidden_size})
Linear("k_proj", ["input"], {"out_features":hidden_size//num_attention_heads*num_key_value_heads})
Linear("v_proj", ["input"], {"out_features":hidden_size//num_attention_heads*num_key_value_heads})
ReshapeTranspose("q_reshape", ["q_proj"], {"shape":["input_shape[0]",num_attention_heads,"input_shape[2]", hidden_size//num_attention_heads]})
ReshapeTranspose("k_reshape", ["k_proj"], {"shape":["input_shape[0]",num_key_value_heads,hidden_size//num_attention_heads,"input_shape[2]"]})
ReshapeTranspose("v_reshape", ["v_proj"], {"shape":["input_shape[0]",num_key_value_heads,"input_shape[2]", hidden_size//num_attention_heads]})
MatMul("qk_matmul", ["q_reshape", "k_reshape"])
Softmax("softmax", ["qk_matmul"])
MatMul("sv_matmul", ["softmax", "v_reshape"])
ReshapeTranspose("sv_reshape", ["sv_matmul"], {"shape":["input_shape[0]","input_shape[2]", hidden_size]})
Linear("out_proj", ["sv_reshape"], {"out_features":hidden_size})
Add("attn_add", ["input", "out_proj"])
Norm("mlp_norm", ["attn_add"])
Linear("gate_proj", ["mlp_norm"], {"out_features":intermediate_size})
Linear("up_proj", ["mlp_norm"], {"out_features":intermediate_size})
Activation("mlp_act", ["up_proj", "gate_proj"])